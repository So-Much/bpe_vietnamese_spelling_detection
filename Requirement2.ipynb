{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6fHU9jSQijxfmCgfcSUrR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fm1gUREt1MQm","executionInfo":{"status":"ok","timestamp":1716013177740,"user_tz":-420,"elapsed":12843,"user":{"displayName":"Nhiều Minh","userId":"08783015489968813102"}},"outputId":"4fb3f574-7f49-4a03-db66-9aba88dab472"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install transformers tokenizers datasets\n","\n","from datasets import load_dataset\n","from tokenizers import ByteLevelBPETokenizer\n","from transformers import RobertaTokenizerFast"]},{"cell_type":"code","source":["# Tải tập dữ liệu\n","dataset = load_dataset(\"PaulTran/vietnamese_spelling_error_detection\")\n","\n","# Trích xuất các văn bản từ cột 'input_text' và 'target_text'\n","texts = dataset['train']['input_text'] + dataset['train']['target_text']\n","\n","# Lưu văn bản vào tệp tạm thời để huấn luyện tokenizer\n","with open(\"temp_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n","    for text in texts:\n","        f.write(text + \"\\n\")\n","tokenizer = ByteLevelBPETokenizer()\n","tokenizer.train(files=[\"temp_texts.txt\"], vocab_size=30_000, min_frequency=2, special_tokens=[\n","    \"<s>\",\n","    \"<pad>\",\n","    \"</s>\",\n","    \"<unk>\",\n","    \"<mask>\",\n","])\n"],"metadata":{"id":"bSBLzz3l1Qr-","executionInfo":{"status":"ok","timestamp":1716013415835,"user_tz":-420,"elapsed":238102,"user":{"displayName":"Nhiều Minh","userId":"08783015489968813102"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Lưu tokenizer\n","import os\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(\"tokenizer\", exist_ok=True)\n","\n","try:\n","  tokenizer.save_model(\"tokenizer\")\n","  print(\"Tokenizer saved successfully!\")\n","except OSError as e:\n","  print(f\"Error saving tokenizer: {e}\")\n","\n","# Tạo RobertaTokenizerFast từ tokenizer đã huấn luyện\n","tokenizer = RobertaTokenizerFast.from_pretrained(\"tokenizer\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"SFBfMemd1YAn","executionInfo":{"status":"error","timestamp":1716013631060,"user_tz":-420,"elapsed":730,"user":{"displayName":"Nhiều Minh","userId":"08783015489968813102"}},"outputId":"9443157d-1e07-4d19-8d70-769c4a78856e"},"execution_count":8,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'RobertaTokenizerFast' object has no attribute 'save_model'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-6d843412a7aa>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenizer saved successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'RobertaTokenizerFast' object has no attribute 'save_model'"]}]},{"cell_type":"code","source":["# Kiểm tra tokenizer mới\n","text = dataset['train']['input_text'][10]\n","encoded_input = tokenizer.encode(text)\n","print(\"Mã hóa văn bản:\")\n","print(len(encoded_input))\n","print(encoded_input)\n","decoded_text = tokenizer.decode(encoded_input)\n","print(\"\\nGiải mã văn bản:\")\n","print(decoded_text)\n","print(len(decoded_text.split(' ')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ms05wn2yYnES","executionInfo":{"status":"ok","timestamp":1716013764372,"user_tz":-420,"elapsed":343,"user":{"displayName":"Nhiều Minh","userId":"08783015489968813102"}},"outputId":"a3e5604d-6208-4a75-c47f-fdf3958f7d37"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mã hóa văn bản:\n","47\n","[0, 1607, 1235, 1656, 2719, 10211, 861, 705, 432, 625, 1657, 1077, 776, 441, 1066, 1656, 787, 476, 497, 764, 16, 1103, 476, 7059, 1087, 364, 11777, 17, 11672, 3243, 9976, 17, 1460, 16, 4493, 17, 12045, 16, 10119, 421, 303, 1764, 494, 7177, 757, 1856, 2]\n","\n","Giải mã văn bản:\n","<s>Các cầu thủ U23 Việt Nam làm nên kỳ tích trước các đối thủ lớn về thể hình, mạnh về tốc độ như Uzơ-bê–kit- tan, I-rắc, Quata cũng là nhờ vàoqa bản lĩnh</s>\n","33\n"]}]}]}